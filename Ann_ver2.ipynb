{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ann_ver2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dYr-_oZSyBAN"},"outputs":[],"source":["# Import libraries and packages\n","import matplotlib as plt\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","%matplotlib inline\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from keras.models import Sequential\n","from keras.layers.core import Dense\n","from tensorflow.keras.optimizers import SGD\n","from imutils import paths\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import pickle\n","import cv2\n","import os\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from tensorflow.keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n","from keras.losses import categorical_crossentropy\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.optimizers import Adam # - Works\n","from keras.regularizers import l2\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import time   # time1 = time.time(); print('Time taken: {:.1f} seconds'.format(time.time() - time1))\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","SEED = 42   # set random seed"]},{"cell_type":"code","source":["# initialize the data and labels\n","print(\"[INFO] loading images...\")\n","time1 = time.time()   # to measure time taken\n","data = []\n","labels = []\n","\n","# grab the image paths and randomly shuffle them\n","imagePaths = sorted(list(paths.list_images(# initialize the data and labels\n","print(\"[INFO] loading images...\")\n","images = []\n","labels = []\n","\n","# grab the image paths and randomly shuffle them\n","imagePaths = sorted(list(paths.list_images('animals')))\n","random.seed(42)\n","random.shuffle(imagePaths)\n","\n","# loop over the input images\n","for imagePath in imagePaths:\n","    # load the image, resize it to 64x64 pixels (the required input spatial dimensions of SmallVGGNet), \n","    # and store the image in the data list\n","    image = cv2.imread(imagePath)\n","    image = cv2.resize(image, (64, 64))   # we are not flattening our data for neural network, because it is convolutional\n","    images.append(image)\n","\n","    # extract the class label from the image path and update the labels list\n","    label = imagePath.split(os.path.sep)[-2]\n","    labels.append(label)\n","\n","# scale the raw pixel intensities to the range [0, 1]\n","images = np.array(images, dtype=\"float\") / 255.0\n","labels = np.array(labels)\n","\n","print('done'))))   # data folder with 3 categorical folders\n","random.seed(SEED)\n","random.shuffle(imagePaths)\n","\n","# loop over the input images\n","for imagePath in imagePaths:\n","    # load the image, resize the image to be 32x32 pixels (ignoring aspect ratio), \n","    # flatten the 32x32x3=3072 pixel image into a list, and store the image in the data list\n","    image = cv2.imread(imagePath)\n","    image = cv2.resize(image, (128, 128)).flatten()\n","    data.append(image)\n"," \n","    # extract the class label from the image path and update the labels list\n","    label = imagePath.split(os.path.sep)[-2]\n","    labels.append(label)\n","\n","# scale the raw pixel intensities to the range [0, 1]\n","data = np.array(data, dtype=\"float\") / 255.0\n","labels = np.array(labels)\n","\n","print('Time taken: {:.1f} seconds'.format(time.time() - time1))   # to measure time taken\n","print(\"done\")"],"metadata":{"id":"ed4VnxrW1uWH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650695798717,"user_tz":-420,"elapsed":409,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}},"outputId":"d329cf93-9961-4473-fbd7-c1f1ef75170c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] loading images...\n","Time taken: 0.1 seconds\n","done\n"]}]},{"cell_type":"code","source":["# partition the data into 80% training and 20% validation\n","(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.2, random_state=SEED)"],"metadata":{"id":"h4CTVRK012FJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q99H99ycLT0S","executionInfo":{"status":"ok","timestamp":1650696480931,"user_tz":-420,"elapsed":18,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}},"outputId":"8a33d685-8ad9-4523-9d36-9e48ec786f39"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28, 49152)"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# convert the labels from integers/categories to vectors \n","# (for 2-class, binary classification you should use Keras' to_categorical function instead)\n","lb = LabelBinarizer()\n","Y_train = lb.fit_transform(Y_train)   # fit_transform = find all unique class labels + transform into one-hot encoded labels\n","Y_test = lb.transform(Y_test)         # transform = perform the one-hot encoding (unique class labels already found)\n","\n","# This is the categorical vector after transformation\n","# [1, 0, 0] # corresponds to cat\n","# [0, 1, 0] # corresponds to dog\n","# [0, 0, 1] # corresponds to panda"],"metadata":{"id":"b4ikhT4q14sz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# smallvggnet.py\n","\n","# import the necessary packages\n","import keras\n","from keras.models import Sequential\n","from tensorflow.keras.layers import BatchNormalization\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.core import Activation\n","from keras.layers.core import Flatten\n","from keras.layers.core import Dropout\n","from keras.layers.core import Dense\n","from keras import backend as K\n","import time \n","# define our SmallVGGNet class and the build method\n","# Should I use \"ELU\" for hidden layer as better for image classification?\n","class SmallVGGNet:\n","    @staticmethod\n","    def build(width, height, depth, classes):\n","        # initialize the model along with the input shape to be \"channels last\" and the channels dimension/depth itself\n","        model = Sequential()   # (i.e. TensorFlow ordering)\n","        inputShape = (height, width, depth)\n","        chanDim = -1\n","\n","        # if we are using \"channels first\", update the input shape and channels dimension\n","        if K.image_data_format() == \"channels_first\":   # (i.e. Theano ordering)\n","            inputShape = (depth, height, width)\n","            chanDim = 1\n","\n","        # CONV => RELU => POOL layer set              # first CONV layer has 32 filters of size 3x3\n","        model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\n","        model.add(Activation(\"relu\"))                 # ReLU (Rectified Linear Unit) activation function\n","        model.add(BatchNormalization(axis=chanDim))   # normalize activations of input volume before passing to next layer\n","        model.add(MaxPooling2D(pool_size=(2, 2)))     # progressively reduce spatial size (width and height) of input \n","        model.add(Dropout(0.25))                      # disconnecting random neurons between layers, reduce overfitting\n","\n","        # (CONV => RELU) * 2 => POOL layer set          # filter dimensions remain the same (3x3)\n","        model.add(Conv2D(64, (3, 3), padding=\"same\"))   # increase total number of filters learned (from 32 to 64)\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        # (CONV => RELU) * 3 => POOL layer set\n","        model.add(Conv2D(128, (3, 3), padding=\"same\"))   # total number of filters learned by CONV layers has doubled (128)\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(MaxPooling2D(pool_size=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        # first (and only) set of fully connected layer (FC) => RELU layers\n","        model.add(Flatten())\n","        model.add(Dense(512))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.5))\n","\n","        # softmax classifier\n","        model.add(Dense(classes))\n","        model.add(Activation(\"softmax\"))\n","\n","        # return the constructed network architecture\n","        return model"],"metadata":{"id":"QRkY3xYM189y","executionInfo":{"status":"ok","timestamp":1650787422837,"user_tz":-420,"elapsed":10,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# import the necessary packages\n","# from pyimagesearch.smallvggnet import SmallVGGNet   # \"smallvggnet.py\" file is in \"pyimagesearch\" folder\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import SGD\n","from imutils import paths\n","import matplotlib as plt\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import random\n","import pickle\n","import cv2\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"TMEwls53QAFW","executionInfo":{"status":"ok","timestamp":1650787425342,"user_tz":-420,"elapsed":387,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# initialize the data and labels\n","print(\"[INFO] loading images...\")\n","images = []\n","labels = []\n","\n","# grab the image paths and randomly shuffle them\n","imagePaths = sorted(list(paths.list_images('/content/drive/MyDrive/Colab Notebooks/img6')))\n","random.seed(42)\n","random.shuffle(imagePaths)\n","\n","# loop over the input images\n","for imagePath in imagePaths:\n","    # load the image, resize it to 64x64 pixels (the required input spatial dimensions of SmallVGGNet), \n","    # and store the image in the data list\n","    image = cv2.imread(imagePath)\n","    image = cv2.resize(image, (64, 64))   # we are not flattening our data for neural network, because it is convolutional\n","    images.append(image)\n","\n","    # extract the class label from the image path and update the labels list\n","    label = imagePath.split(os.path.sep)[-2]\n","    labels.append(label)\n","\n","# scale the raw pixel intensities to the range [0, 1]\n","images = np.array(images, dtype=\"float\") / 255.0\n","labels = np.array(labels)\n","\n","print('done')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKFALzIyQJtw","executionInfo":{"status":"ok","timestamp":1650787426856,"user_tz":-420,"elapsed":31,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}},"outputId":"85e47b6e-7ede-4579-a44c-38566de9873b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] loading images...\n","done\n"]}]},{"cell_type":"code","source":["# partition the data into 75% training and 25% validation, try 80/20 later\n","(trainX, testX, trainY, testY) = train_test_split(images,labels, test_size=0.2, random_state=42)\n","\n","# before transformation\n","trainY"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"78SbbinRQoyT","executionInfo":{"status":"ok","timestamp":1650787429052,"user_tz":-420,"elapsed":12,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}},"outputId":"c58d3c95-a4f5-411d-9025-a0492484564e"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['2', '5', '6', '4', '0', '1', '6', '1', '0', '4', '2', '2', '1',\n","       '3', '4', '6', '0', '3', '2', '3', '5', '5', '5', '6', '1', '3',\n","       '1', '4'], dtype='<U1')"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["# convert the labels from integers to vectors \n","# (for 2-class, binary classification you should use Keras' to_categorical function)\n","lb = LabelBinarizer()\n","trainY = lb.fit_transform(trainY)\n","testY = lb.transform(testY)\n","\n","# after transformation\n","trainY"],"metadata":{"id":"4G-cvTAbQru2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","# Construct & initialize the image data generator for data augmentation\n","# Image augmentation allows us to construct “additional” training data from our existing training data \n","# by randomly rotating, shifting, shearing, zooming, and flipping. This is to avoid overfitting.\n","aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n","                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n","                         horizontal_flip=True, fill_mode=\"nearest\")\n","\n","# initialize our VGG-like Convolutional Neural Network\n","model = SmallVGGNet.build(width=64, height=64, depth=3, classes=len(lb.classes_))\n","\n","# compile & train model\n","# initialize our initial learning rate, # of epochs to train for, and batch size\n","INIT_LR = 0.01\n","EPOCHS = 30\n","BS = 32\n","\n","# initialize the model and optimizer (you'll want to use binary_crossentropy for 2-class classification)\n","print(\"[INFO] training network...\")\n","opt = SGD(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"],run_eagerly=True)\n","\n","# train the network\n","H = model.fit_generator( aug.flow(trainX, trainY, batch_size=BS),\n","                        validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS, epochs=EPOCHS,\n","                        callbacks=[keras.callbacks.EarlyStopping(patience=11, verbose=1, restore_best_weights=True),\n","                                   keras.callbacks.ReduceLROnPlateau(factor=.5, patience=4, verbose=1)] )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"ouJcoq30QuhN","executionInfo":{"status":"ok","timestamp":1650787591157,"user_tz":-420,"elapsed":477,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}},"outputId":"db73f6dc-2647-4d78-bcb1-48ab34284483"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] training network...\n"]},{"output_type":"execute_result","data":{"text/plain":["'H = model.fit_generator( aug.flow(trainX, trainY, batch_size=BS),\\n                        validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS, epochs=EPOCHS,\\n                        callbacks=[keras.callbacks.EarlyStopping(patience=11, verbose=1, restore_best_weights=True),\\n                                   keras.callbacks.ReduceLROnPlateau(factor=.5, patience=4, verbose=1)] )'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# performing data argumentation by training image generator\n","dataAugmentaion = ImageDataGenerator(rotation_range = 30, zoom_range = 0.20, \n","fill_mode = \"nearest\", shear_range = 0.20, horizontal_flip = True, \n","width_shift_range = 0.1, height_shift_range = 0.1)\n","\n","# training the model\n","model.fit_generator(dataAugmentaion.flow(trainX, trainY, batch_size = 32),\n"," validation_data = (testX, testY), steps_per_epoch = len(trainX) // 32,\n"," epochs = 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"C-xe5KLcm6vX","executionInfo":{"status":"error","timestamp":1650787662968,"user_tz":-420,"elapsed":444,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}},"outputId":"67d57ccd-d92a-4f22-86d9-1f4eadf7a391"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-8a7dc676d18b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m model.fit_generator(dataAugmentaion.flow(trainX, trainY, batch_size = 32),\n\u001b[1;32m      8\u001b[0m  \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m  epochs = 10)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2221\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2223\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m           raise ValueError('Unexpected result of `train_function` '\n\u001b[0m\u001b[1;32m   1396\u001b[0m                            \u001b[0;34m'(Empty logs). Please use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                            \u001b[0;34m'`Model.compile(..., run_eagerly=True)`, or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."]}]},{"cell_type":"code","source":["# train the neural network on training data set\n","# batch_size (32) controls the size of each group of data to pass through the network. \n","# Larger GPUs would be able to accommodate larger batch sizes (eg. 64)\n","time1 = time.time()   # to measure time taken\n","H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=EPOCHS, batch_size=BS)\n","print('Time taken: {:.1f} seconds'.format(time.time() - time1))   # to measure time taken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMSY8IN0mHOC","executionInfo":{"status":"ok","timestamp":1650787577754,"user_tz":-420,"elapsed":142048,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}},"outputId":"a577868c-f8be-4bca-f0a7-8ed2e205b281"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/75\n","1/1 [==============================] - 2s 2s/step - loss: 3.2669 - accuracy: 0.0714 - val_loss: 1.9412 - val_accuracy: 0.0000e+00\n","Epoch 2/75\n","1/1 [==============================] - 1s 1s/step - loss: 2.7623 - accuracy: 0.1429 - val_loss: 1.9400 - val_accuracy: 0.2857\n","Epoch 3/75\n","1/1 [==============================] - 1s 1s/step - loss: 2.1223 - accuracy: 0.3571 - val_loss: 1.9380 - val_accuracy: 0.1429\n","Epoch 4/75\n","1/1 [==============================] - 1s 1s/step - loss: 2.0749 - accuracy: 0.4286 - val_loss: 1.9430 - val_accuracy: 0.1429\n","Epoch 5/75\n","1/1 [==============================] - 1s 1s/step - loss: 1.4863 - accuracy: 0.4643 - val_loss: 1.9472 - val_accuracy: 0.1429\n","Epoch 6/75\n","1/1 [==============================] - 1s 1s/step - loss: 1.1807 - accuracy: 0.5357 - val_loss: 1.9390 - val_accuracy: 0.1429\n","Epoch 7/75\n","1/1 [==============================] - 1s 1s/step - loss: 1.1216 - accuracy: 0.6429 - val_loss: 1.9360 - val_accuracy: 0.1429\n","Epoch 8/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.7689 - accuracy: 0.7500 - val_loss: 1.9394 - val_accuracy: 0.1429\n","Epoch 9/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.8006 - accuracy: 0.7500 - val_loss: 1.9472 - val_accuracy: 0.1429\n","Epoch 10/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.5080 - accuracy: 0.8214 - val_loss: 1.9498 - val_accuracy: 0.1429\n","Epoch 11/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.4394 - accuracy: 0.8214 - val_loss: 1.9613 - val_accuracy: 0.1429\n","Epoch 12/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.2173 - accuracy: 1.0000 - val_loss: 1.9730 - val_accuracy: 0.1429\n","Epoch 13/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.5139 - accuracy: 0.7857 - val_loss: 1.9785 - val_accuracy: 0.1429\n","Epoch 14/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.4538 - accuracy: 0.8214 - val_loss: 1.9866 - val_accuracy: 0.1429\n","Epoch 15/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.2491 - accuracy: 0.8929 - val_loss: 1.9937 - val_accuracy: 0.1429\n","Epoch 16/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.1950 - accuracy: 0.9286 - val_loss: 1.9926 - val_accuracy: 0.1429\n","Epoch 17/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.2130 - accuracy: 0.9286 - val_loss: 2.0055 - val_accuracy: 0.1429\n","Epoch 18/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.2073 - accuracy: 0.9286 - val_loss: 2.0231 - val_accuracy: 0.1429\n","Epoch 19/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.2051 - accuracy: 0.9643 - val_loss: 2.0317 - val_accuracy: 0.1429\n","Epoch 20/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.2578 - accuracy: 0.8571 - val_loss: 2.0457 - val_accuracy: 0.1429\n","Epoch 21/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.1451 - accuracy: 0.9286 - val_loss: 2.0542 - val_accuracy: 0.1429\n","Epoch 22/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 2.0596 - val_accuracy: 0.1429\n","Epoch 23/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.1147 - accuracy: 0.9643 - val_loss: 2.0652 - val_accuracy: 0.1429\n","Epoch 24/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0916 - accuracy: 0.9643 - val_loss: 2.0741 - val_accuracy: 0.1429\n","Epoch 25/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 2.0868 - val_accuracy: 0.1429\n","Epoch 26/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 2.0965 - val_accuracy: 0.1429\n","Epoch 27/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.1160 - accuracy: 0.9643 - val_loss: 2.1177 - val_accuracy: 0.1429\n","Epoch 28/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 2.1327 - val_accuracy: 0.1429\n","Epoch 29/75\n","1/1 [==============================] - 3s 3s/step - loss: 0.0986 - accuracy: 0.9643 - val_loss: 2.1483 - val_accuracy: 0.1429\n","Epoch 30/75\n","1/1 [==============================] - 3s 3s/step - loss: 0.0875 - accuracy: 1.0000 - val_loss: 2.1712 - val_accuracy: 0.1429\n","Epoch 31/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.0961 - accuracy: 0.9643 - val_loss: 2.1852 - val_accuracy: 0.1429\n","Epoch 32/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 2.1992 - val_accuracy: 0.1429\n","Epoch 33/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.0788 - accuracy: 0.9643 - val_loss: 2.2228 - val_accuracy: 0.1429\n","Epoch 34/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 2.2366 - val_accuracy: 0.1429\n","Epoch 35/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 2.2528 - val_accuracy: 0.1429\n","Epoch 36/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.1554 - accuracy: 0.9286 - val_loss: 2.2687 - val_accuracy: 0.1429\n","Epoch 37/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 2.2868 - val_accuracy: 0.1429\n","Epoch 38/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 2.3087 - val_accuracy: 0.1429\n","Epoch 39/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 2.3261 - val_accuracy: 0.1429\n","Epoch 40/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 2.3432 - val_accuracy: 0.1429\n","Epoch 41/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 2.3631 - val_accuracy: 0.1429\n","Epoch 42/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 2.3837 - val_accuracy: 0.1429\n","Epoch 43/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0775 - accuracy: 1.0000 - val_loss: 2.3973 - val_accuracy: 0.1429\n","Epoch 44/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 2.4170 - val_accuracy: 0.1429\n","Epoch 45/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0969 - accuracy: 0.9643 - val_loss: 2.4345 - val_accuracy: 0.1429\n","Epoch 46/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 2.4510 - val_accuracy: 0.1429\n","Epoch 47/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 2.4644 - val_accuracy: 0.1429\n","Epoch 48/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0384 - accuracy: 0.9643 - val_loss: 2.4721 - val_accuracy: 0.1429\n","Epoch 49/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 2.4928 - val_accuracy: 0.1429\n","Epoch 50/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 2.5136 - val_accuracy: 0.1429\n","Epoch 51/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0623 - accuracy: 1.0000 - val_loss: 2.5329 - val_accuracy: 0.1429\n","Epoch 52/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 2.5520 - val_accuracy: 0.1429\n","Epoch 53/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 2.5735 - val_accuracy: 0.1429\n","Epoch 54/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 2.5922 - val_accuracy: 0.1429\n","Epoch 55/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 2.6162 - val_accuracy: 0.1429\n","Epoch 56/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.6340 - val_accuracy: 0.1429\n","Epoch 57/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 2.6563 - val_accuracy: 0.1429\n","Epoch 58/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 2.6776 - val_accuracy: 0.1429\n","Epoch 59/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 2.6949 - val_accuracy: 0.1429\n","Epoch 60/75\n","1/1 [==============================] - 2s 2s/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 2.7172 - val_accuracy: 0.1429\n","Epoch 61/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0415 - accuracy: 1.0000 - val_loss: 2.7280 - val_accuracy: 0.1429\n","Epoch 62/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 2.7491 - val_accuracy: 0.1429\n","Epoch 63/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 2.7696 - val_accuracy: 0.1429\n","Epoch 64/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0480 - accuracy: 0.9643 - val_loss: 2.8000 - val_accuracy: 0.1429\n","Epoch 65/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 2.8220 - val_accuracy: 0.1429\n","Epoch 66/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 2.8425 - val_accuracy: 0.1429\n","Epoch 67/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 2.8536 - val_accuracy: 0.1429\n","Epoch 68/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 2.8525 - val_accuracy: 0.1429\n","Epoch 69/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 2.8740 - val_accuracy: 0.1429\n","Epoch 70/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 2.8921 - val_accuracy: 0.1429\n","Epoch 71/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0624 - accuracy: 0.9643 - val_loss: 2.9071 - val_accuracy: 0.1429\n","Epoch 72/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.9246 - val_accuracy: 0.1429\n","Epoch 73/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 2.9417 - val_accuracy: 0.1429\n","Epoch 74/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 2.9669 - val_accuracy: 0.1429\n","Epoch 75/75\n","1/1 [==============================] - 1s 1s/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 3.0011 - val_accuracy: 0.1429\n","Time taken: 142.0 seconds\n"]}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCZp5Q9yngB1","executionInfo":{"status":"ok","timestamp":1650787748498,"user_tz":-420,"elapsed":1011,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}},"outputId":"8966190e-daf6-4908-f9da-4918246ea745"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_48 (Conv2D)          (None, 64, 64, 32)        896       \n","                                                                 \n"," activation_64 (Activation)  (None, 64, 64, 32)        0         \n","                                                                 \n"," batch_normalization_56 (Bat  (None, 64, 64, 32)       128       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_24 (MaxPoolin  (None, 32, 32, 32)       0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_32 (Dropout)        (None, 32, 32, 32)        0         \n","                                                                 \n"," conv2d_49 (Conv2D)          (None, 32, 32, 64)        18496     \n","                                                                 \n"," activation_65 (Activation)  (None, 32, 32, 64)        0         \n","                                                                 \n"," batch_normalization_57 (Bat  (None, 32, 32, 64)       256       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_50 (Conv2D)          (None, 32, 32, 64)        36928     \n","                                                                 \n"," activation_66 (Activation)  (None, 32, 32, 64)        0         \n","                                                                 \n"," batch_normalization_58 (Bat  (None, 32, 32, 64)       256       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_25 (MaxPoolin  (None, 16, 16, 64)       0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_33 (Dropout)        (None, 16, 16, 64)        0         \n","                                                                 \n"," conv2d_51 (Conv2D)          (None, 16, 16, 128)       73856     \n","                                                                 \n"," activation_67 (Activation)  (None, 16, 16, 128)       0         \n","                                                                 \n"," batch_normalization_59 (Bat  (None, 16, 16, 128)      512       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_52 (Conv2D)          (None, 16, 16, 128)       147584    \n","                                                                 \n"," activation_68 (Activation)  (None, 16, 16, 128)       0         \n","                                                                 \n"," batch_normalization_60 (Bat  (None, 16, 16, 128)      512       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_53 (Conv2D)          (None, 16, 16, 128)       147584    \n","                                                                 \n"," activation_69 (Activation)  (None, 16, 16, 128)       0         \n","                                                                 \n"," batch_normalization_61 (Bat  (None, 16, 16, 128)      512       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_26 (MaxPoolin  (None, 8, 8, 128)        0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_34 (Dropout)        (None, 8, 8, 128)         0         \n","                                                                 \n"," flatten_8 (Flatten)         (None, 8192)              0         \n","                                                                 \n"," dense_16 (Dense)            (None, 512)               4194816   \n","                                                                 \n"," activation_70 (Activation)  (None, 512)               0         \n","                                                                 \n"," batch_normalization_62 (Bat  (None, 512)              2048      \n"," chNormalization)                                                \n","                                                                 \n"," dropout_35 (Dropout)        (None, 512)               0         \n","                                                                 \n"," dense_17 (Dense)            (None, 7)                 3591      \n","                                                                 \n"," activation_71 (Activation)  (None, 7)                 0         \n","                                                                 \n","=================================================================\n","Total params: 4,627,975\n","Trainable params: 4,625,863\n","Non-trainable params: 2,112\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# evaluate the network\n","print(\"[INFO] evaluating network...\")\n","predictions = model.predict(testX, batch_size=32)\n","print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_))"],"metadata":{"id":"bvTPcSXd2D3q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the training and validation loss\n","N = np.arange(0, EPOCHS)\n","plt.style.use(\"ggplot\")\n","plt.figure(figsize = [10,8])\n","plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n","plt.title(\"Training & Validation Loss (Simple NN)\")\n","plt.xlabel(\"Epoch #\", weight=\"bold\")\n","plt.ylabel(\"Loss\", weight=\"bold\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"CJYxvGv82FWE","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"error","timestamp":1650788540510,"user_tz":-420,"elapsed":18,"user":{"displayName":"วงศกร กองกะมุด","userId":"10058041215998170645"}},"outputId":"a4a53ecb-b306-40bf-c803-c578598026ef"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c7de48d8f028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot the training and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ggplot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"code","source":["# plot the training and validation accuracy\n","N = np.arange(0, EPOCHS)\n","plt.style.use(\"ggplot\")\n","plt.figure(figsize = [10,8])\n","plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n","plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n","plt.title(\"Training and Validation Accuracy (Simple NN)\")\n","plt.xlabel(\"Epoch #\", weight=\"bold\")\n","plt.ylabel(\"Accuracy\", weight=\"bold\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"0I4TSg9A2HES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"t4oHhi-42JHt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the model and label binarizer to disk\n","print(\"[INFO] serializing network and label binarizer...\")\n","model.save('simple_NN_model.h5')\n","f = open('simple_NN_label_bin', \"wb\")\n","f.write(pickle.dumps(lb))\n","f.close()"],"metadata":{"id":"JFx0akPL2Kk5"},"execution_count":null,"outputs":[]}]}